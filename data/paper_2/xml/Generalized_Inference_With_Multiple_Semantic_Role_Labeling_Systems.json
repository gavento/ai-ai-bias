{
    "item_type": "paper",
    "title": "Generalized Inference With Multiple Semantic Role Labeling Systems",
    "abstract": "result with joint inference on the development set. Overall results on the development and test sets are shown in Table 1. Table 2 shows the results of individual systems and the improvement gained by the joint inference on the development set. 4 Conclusions We present an implementation of SRL system which composed of four stages—1) pruning, 2) argument identification, 3) argument classification, and 4) inference. The inference provides a natural way to take the output of multiple argument classifiers and combines them into a coherent predicate-argument output. Significant improvement in overall SRL performance through this inference is illustrated. Acknowledgments We are grateful to Dash Optimization for the free academic use of Xpress-MP. This research is sup",
    "abstract_xml": "<ABSTRACT>\n    <S sid=\"1\" ssid=\"1\">result with joint inference on the development set.</S>\n    <S sid=\"2\" ssid=\"2\">Overall results on the development and test sets are shown in Table 1.</S>\n    <S sid=\"3\" ssid=\"3\">Table 2 shows the results of individual systems and the improvement gained by the joint inference on the development set.</S>\n    <S sid=\"4\" ssid=\"4\">4 Conclusions We present an implementation of SRL system which composed of four stages—1) pruning, 2) argument identification, 3) argument classification, and 4) inference.</S>\n    <S sid=\"5\" ssid=\"5\">The inference provides a natural way to take the output of multiple argument classifiers and combines them into a coherent predicate-argument output.</S>\n    <S sid=\"6\" ssid=\"6\">Significant improvement in overall SRL performance through this inference is illustrated.</S>\n    <S sid=\"7\" ssid=\"7\">Acknowledgments We are grateful to Dash Optimization for the free academic use of Xpress-MP.</S>\n    <S sid=\"8\" ssid=\"8\">This research is sup</S>\n  </ABSTRACT>\n  ",
    "origin": "Human",
    "article": "<PAPER>\n  <S sid=\"0\">Generalized Inference With Multiple Semantic Role Labeling Systems</S>\n  <SECTION title=\"1 SRL System Architecture\" number=\"1\">\n    <S sid=\"9\" ssid=\"1\">Our SRL system consists of four stages: pruning, argument identification, argument classification, and inference.</S>\n    <S sid=\"10\" ssid=\"2\">In particular, the goal of pruning and argument identification is to identify argument candidates for a given verb predicate.</S>\n    <S sid=\"11\" ssid=\"3\">The system only classifies the argument candidates into their types during the argument classification stage.</S>\n    <S sid=\"12\" ssid=\"4\">Linguistic and structural constraints are incorporated in the inference stage to resolve inconsistent global predictions.</S>\n    <S sid=\"13\" ssid=\"5\">The inference stage can take as its input the output of the argument classification of a single system or of multiple systems.</S>\n    <S sid=\"14\" ssid=\"6\">We explain the inference for multiple systems in Sec.</S>\n    <S sid=\"15\" ssid=\"7\">2.</S>\n    <S sid=\"16\" ssid=\"8\">Only the constituents in the parse tree are considered as argument candidates.</S>\n    <S sid=\"17\" ssid=\"9\">In addition, our system exploits the heuristic introduced by (Xue and Palmer, 2004) to filter out very unlikely constituents.</S>\n    <S sid=\"18\" ssid=\"10\">The heuristic is a recursive process starting from the verb whose arguments are to be identified.</S>\n    <S sid=\"19\" ssid=\"11\">It first returns the siblings of the verb; then it moves to the parent of the verb, and collects the siblings again.</S>\n    <S sid=\"20\" ssid=\"12\">The process goes on until it reaches the root.</S>\n    <S sid=\"21\" ssid=\"13\">In addition, if a constituent is a PP (propositional phrase), its children are also collected.</S>\n    <S sid=\"22\" ssid=\"14\">Candidates consisting of only a single punctuation mark are not considered.</S>\n    <S sid=\"23\" ssid=\"15\">This heuristic works well with the correct parse trees.</S>\n    <S sid=\"24\" ssid=\"16\">However, one of the errors by automatic parsers is due to incorrect PP attachment leading to missing arguments.</S>\n    <S sid=\"25\" ssid=\"17\">To attempt to fix this, we consider as arguments the combination of any consecutive NP and PP, and the split of NP and PP inside the NP that was chosen by the previous heuristics.</S>\n    <S sid=\"26\" ssid=\"18\">The argument identification stage utilizes binary classification to identify whether a candidate is an argument or not.</S>\n    <S sid=\"27\" ssid=\"19\">We train and apply the binary classifiers on the constituents supplied by the pruning stage.</S>\n    <S sid=\"28\" ssid=\"20\">Most of the features used in our system are standard features, which include This stage assigns the final argument labels to the argument candidates supplied from the previous stage.</S>\n    <S sid=\"29\" ssid=\"21\">A multi-class classifier is trained to classify the types of the arguments supplied by the argument identification stage.</S>\n    <S sid=\"30\" ssid=\"22\">To reduce the excessive candidates mistakenly output by the previous stage, the classifier can also classify the argument as NULL (“not an argument”) to discard the argument.</S>\n    <S sid=\"31\" ssid=\"23\">The features used here are the same as those used in the argument identification stage with the following additional features.</S>\n    <S sid=\"32\" ssid=\"24\">The purpose of this stage is to incorporate some prior linguistic and structural knowledge, such as “arguments do not overlap” or “each verb takes at most one argument of each type.” This knowledge is used to resolve any inconsistencies of argument classification in order to generate final legitimate predictions.</S>\n    <S sid=\"33\" ssid=\"25\">We use the inference process introduced by (Punyakanok et al., 2004).</S>\n    <S sid=\"34\" ssid=\"26\">The process is formulated as an integer linear programming (ILP) problem that takes as inputs the confidences over each type of the arguments supplied by the argument classifier.</S>\n    <S sid=\"35\" ssid=\"27\">The output is the optimal solution that maximizes the linear sum of the confidence scores (e.g., the conditional probabilities estimated by the argument classifier), subject to the constraints that encode the domain knowledge.</S>\n    <S sid=\"36\" ssid=\"28\">Formally speaking, the argument classifier attempts to assign labels to a set of arguments, S1:M, indexed from 1 to M. Each argument Si can take any label from a set of argument labels, P, and the indexed set of arguments can take a set of labels, c1:M E PM.</S>\n    <S sid=\"37\" ssid=\"29\">If we assume that the argument classifier returns an estimated conditional probability distribution, Prob(Si = ci), then, given a sentence, the inference procedure seeks an global assignment that maximizes the following objective function, subject to linguistic and structural constraints.</S>\n    <S sid=\"38\" ssid=\"30\">In other words, this objective function reflects the expected number of correct argument predictions, subject to the constraints.</S>\n    <S sid=\"39\" ssid=\"31\">The constraints are encoded as the followings.</S>\n  </SECTION>\n  <SECTION title=\"2 Inference with Multiple SRL Systems\" number=\"2\">\n    <S sid=\"40\" ssid=\"1\">The inference process allows a natural way to combine the outputs from multiple argument classifiers.</S>\n    <S sid=\"41\" ssid=\"2\">Specifically, given k argument classifiers which perform classification on k argument sets, {S1, ... , Sk}.</S>\n    <S sid=\"42\" ssid=\"3\">The inference process aims to optimize the objective function: ..., traders say, unable to cool the selling panic in both stocks and futures. where Probj is the probability output by system j.</S>\n    <S sid=\"43\" ssid=\"4\">Note that all systems may not output with the same set of argument candidates due to the pruning and argument identification.</S>\n    <S sid=\"44\" ssid=\"5\">For the systems that do not output for any candidate, we assign the probability with a prior to this phantom candidate.</S>\n    <S sid=\"45\" ssid=\"6\">In particular, the probability of the NULL class is set to be 0.6 based on empirical tests, and the probabilities of the other classes are set proportionally to their occurrence frequencies in the training data.</S>\n    <S sid=\"46\" ssid=\"7\">For example, Figure 1 shows the two candidate sets for a fragment of a sentence, “..., traders say, unable to cool the selling panic in both stocks and futures.” In this example, system A has two argument candidates, a1 = “traders” and a4 = “the selling panic in both stocks and futures”; system B has three argument candidates, b1 = “traders”, b2 = “the selling panic”, and b3 = “in both stocks and futures”.</S>\n    <S sid=\"47\" ssid=\"8\">The phantom candidates are created for a2, a3, and b4 of which probability is set to the prior.</S>\n    <S sid=\"48\" ssid=\"9\">Specifically for this implementation, we first train two SRL systems that use Collins’ parser and Charniak’s parser respectively.</S>\n    <S sid=\"49\" ssid=\"10\">In fact, these two parsers have noticeably different output.</S>\n    <S sid=\"50\" ssid=\"11\">In evaluation, we run the system that was trained with Charniak’s parser 5 times with the top-5 parse trees output by Charniak’s parser1.</S>\n    <S sid=\"51\" ssid=\"12\">Together we have six different outputs per predicate.</S>\n    <S sid=\"52\" ssid=\"13\">Per each parse tree output, we ran the first three stages, namely pruning, argument identification, and argument classification.</S>\n    <S sid=\"53\" ssid=\"14\">Then a joint inference stage is used to resolve the inconsistency of the output of argument classification in these systems.</S>\n  </SECTION>\n  <SECTION title=\"3 Learning and Evaluation\" number=\"3\">\n    <S sid=\"54\" ssid=\"1\">The learning algorithm used is a variation of the Winnow update rule incorporated in SNoW (Roth, 1998; Roth and Yih, 2002), a multi-class classifier that is tailored for large scale learning tasks.</S>\n    <S sid=\"55\" ssid=\"2\">SNoW learns a sparse network of linear functions, in which the targets (argument border predictions or argument type predictions, in this case) are represented as linear functions over a common feature space.</S>\n    <S sid=\"56\" ssid=\"3\">It improves the basic Winnow multiplicative update rule with a regularization term, which has the effect of trying to separate the data with a large margin separator (Grove and Roth, 2001; Hang et al., 2002) and voted (averaged) weight vector (Freund and Schapire, 1999).</S>\n    <S sid=\"57\" ssid=\"4\">Softmax function (Bishop, 1995) is used to convert raw activation to conditional probabilities.</S>\n    <S sid=\"58\" ssid=\"5\">If there are n classes and the raw activation of class i is acti, the posterior estimation for class i is each In summary, training used both full and partial syntactic information as described in Section 1.</S>\n    <S sid=\"59\" ssid=\"6\">In training, SNoW’s default parameters were used with the exception of the separator thickness 1.5, the use of average weight vector, and 5 training cycles.</S>\n    <S sid=\"60\" ssid=\"7\">The parameters are optimized on the development set.</S>\n    <S sid=\"61\" ssid=\"8\">Training for each system took about 6 hours.</S>\n    <S sid=\"62\" ssid=\"9\">The evaluation on both test sets which included running Overall results on the development and test sets are shown in Table 1.</S>\n    <S sid=\"63\" ssid=\"10\">Table 2 shows the results of individual systems and the improvement gained by the joint inference on the development set.</S>\n  </SECTION>\n  <SECTION title=\"4 Conclusions\" number=\"4\">\n    <S sid=\"64\" ssid=\"1\">We present an implementation of SRL system which composed of four stages—1) pruning, 2) argument identification, 3) argument classification, and 4) inference.</S>\n    <S sid=\"65\" ssid=\"2\">The inference provides a natural way to take the output of multiple argument classifiers and combines them into a coherent predicate-argument output.</S>\n    <S sid=\"66\" ssid=\"3\">Significant improvement in overall SRL performance through this inference is illustrated.</S>\n  </SECTION>\n  <SECTION title=\"Acknowledgments\" number=\"5\">\n    <S sid=\"67\" ssid=\"1\">We are grateful to Dash Optimization for the free academic use of Xpress-MP.</S>\n    <S sid=\"68\" ssid=\"2\">This research is supported by ARDA’s AQUAINT Program, DOI’s Reflex program, and an ONR MURI Award.</S>\n  </SECTION>\n</PAPER>",
    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/W05-0625/Documents_xml/W05-0625.xml"
}